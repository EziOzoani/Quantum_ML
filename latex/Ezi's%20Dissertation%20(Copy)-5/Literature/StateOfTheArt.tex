\chapter{State of The Art}\label{StateOArt}
This chapter analyses previous work related to quantum computing and quantum enhanced machine learning. We examine publications that are research based on the one hand and we also examine publications focused on implemented approaches. We compare those with implementations developed in this dissertation. 

Section \ref{GroundState} investigates the relevant work that provides the foundation for quantum machine learning. These publications lay the groundwork for quantum machine learning algorithms through a physics based approach. Moving on, Section \ref{StateImple} examines publications that propose implementations but provide different levels of detail. %This literature review will facilitate us to understand the past work this dissertation will build upon. This will enable us to see how this work is a 
%We will provide 
%a more complete next step, in comparison to section \ref{StateImple}, and is aimed at those who have a more technical background, as it is not as physics based as the first set of research papers in Section \ref{GroundState}. Through the use of current data encoding methods and the greater accessibility of quantum machines, one can further apply these groundwork techniques and evaluate their full potential and their quantum advantage claims.
% using quantum's claimed advantages. Enabling this dissertation to build on this literature review by providing modular
 
 It is hoped that the structures and implementations developed in this dissertation will facilitate a more thorough understanding of these techniques by readers from a computational background. Through the use of current data encoding methods and the greater accessibility of quantum machines, these groundwork techniques can be applied and their full potential and quantum advantages assessed. %Appraise, apprise.

\section{Groundwork}\label{GroundState}
\citep{research1}'s \emph{An introduction to quantum machine learning} provides an overview of the field of quantum machine learning by outlining quantum machine learning algorithms. It provides an analysis of the different quantum approaches that are currently being explored and the potential these approaches have in relation to quantum machine learning. A range of quantum methods such as: quantum machine learning algorithms, quantum neural networks, quantum classification using Bayesian decision theory and hidden quantum Markov models are discussed. Each topic is presented by introducing their classical version and their significance in machine learning. Following this, their quantum counterparts are introduced and an overview of current work is provided. A selection of relevant papers are explored particularly with respect to the implementations developed therein.
%While the other topics discussed in this paper can be used to expand the quantum modular tool,


The quantum machine algorithms QkNN and QSVM developed in the paper are of particular interest. 
Two different QkNN approaches, discussed in \citep{research1} are detailed in the following works: \emph{Machine Learning in a Quantum World} \citep{aimeursubQkNNFid}, \emph{Quantum algorithms for supervised and unsupervised machine learning} \citep{lloydsubQkNNFid} and \emph{Quantum Pattern Recognition} \citep{trugenbergersubQkNNHamm}.

The idea of using the \emph{SWAP test} is introduced in \citeauthor{aimeursubQkNNFid} and \citeauthor{lloydsubQkNNFid}, the \emph{SWAP test} is an inexpensive procedure that estimates the fidelity or overlap in a quantum routine. The \emph{SWAP test} uses the C-\emph{SWAP} test several times on the training set, which encodes the classical length of the vector data into a scalar product for the quantum state, in order to estimate the fidelity between each pair in the set. This allows for the retrieval of the classical distance between two points. As explained in Section \ref{QkNNexplain}, the \emph{SWAP test} is not a true quantum approach, it is instead a quantum enhanced QkNN approach, this is due to its use during the initial clustering, after which a classical clustering algorithm such as k-means is applied on the data that is obtained. This approach allows for the addition of a new data point, as the k-Means latter half finds the nearest centroid for this new input. 
However, \citeauthor{research1} noted both of these papers claim that the \emph{SWAP test} QkNN approach is more efficient than the polynomial run-time of a classical QkNN approach, but they do not make use of a quantum system or a quantum circuit in order to evaluate their results.

The third paper, \citep{trugenbergersubQkNNHamm}, uses a full quantum QkNN approach based on the idea of the Hamming distance,
%As explained in section \ref{QkNNexplain}, 
%distance between two binary quantum states. A quantum superposition of all states of the training set is constructed. The Hamming distance is then
a metric of similarity between binary vectors.

Similar to the other QkNN papers discussed, \citeauthor{trugenbergersubQkNNHamm} provides a mathematical approach for their QkNN operations but does not provide any circuit illustrations for their implementation. It does however discuss the quantum speedup advantage of the Hamming distance QkNN approach, with a focus on the proposed storage advantage when using larger datasets, as compared to classical kNN. As explained in Section \ref{QkNNexplain}, the Hamming distance encodes all distances between the input and the entire training set in one run. As noted by \citep{research1}, the focus on storage optimisation is a different and interesting take on one of the positive outcomes of QkNN, and it will have advantageous implications as the size of data in our digital space continues to grow %[cite]. 
The author also notes that the \emph{``power of this routine only becomes visible if it is applied to a large superposition of training states in the first register''} \citep{research1}. The results chapter in this dissertation in Section \ref{RuntimeOut} illustrates that when QkNN is evaluated on a quantum system, it vastly outperforms kNN when applied to both a small and a larger set of data.

When detailing the current domain of QSVM, there is only one publication listed, \citep{Rebentrost_2014SubSVM} \emph{Quantum Pattern Recognition}. For NISQ (Noisy 
Intermediate-Scale Quantum) devices this QSVM approach is the primary implementation; however, quantum annealing devices \citep{QAnnealing} use binary classification for multi-classification problems. Binary classification models like SVM do not support multi-class classification natively. One way of dealing with this is by splitting the multi-class classification dataset into multiple binary classification datasets and then using a binary classification model on each \citep{demasupport}. %Two commonly used approaches are: one-versus-the-rest and one-versus-one \citep{demasupport}.

\citeauthor{Rebentrost_2014SubSVM}'s QSVM approach is a quantum enhanced approach, and it shows that a Quantum Support Vector Mechanism can be implemented with $ O(log N M)$ run time in both the training and classification stages.
As noted by \citeauthor{research1}, the work by \citeauthor{Rebentrost_2014SubSVM} assumes an oracle for data encoding. It does not describe the encoding process either mathematically or through the use of circuit illustrations.
From the oracle, quantum vectors are returned and evaluated using the inner product evaluation. %Similar to the approach used in section \ref{QSVMImp}
This QSVM approach is a quantum enhanced approach, as these quantum vectors are then prepared for a classical kernel matrix. (This approach is used in the present work in Section \ref{QSVMImp}.)
\citeauthor{research1} note that \emph{``Rebentrost, Mohseni and Lloyd claim that in general, the evaluation of an inner product can be done faster on a quantum computer''} \citep{research1}. While this claim is explored in the results chapter in Section \ref{RuntimeOut} -- which shows that overall this assertion holds true -- \citeauthor{Rebentrost_2014SubSVM} do not make use of a quantum system to evaluate their claim. 

\citep{research1} \emph{An introduction to quantum machine learning} is an introductory step into the different avenues of quantum computing. It details the current work and approaches that have been investigated and their strengths, while also detailing areas that require clarification through re-implementation. The authors found that when exploring quantum machine learning there are two approaches that are taken in different research papers:
 
\begin{enumerate}
\item \emph{``Many authors try to find quantum algorithms that can take the place of classical machine learning algorithms to solve a problem, and show how an improvement in terms of complexity can be gained.''} %\citep{research1}.


\item Others use the probabilistic description of quantum theory in order to describe stochastic processes. This is seen in the quantum hidden Markov models and the Bayesian theory, which helps to generalise these models.
\end{enumerate}

The first point is predominantly true for k-Nearest Neighbour, kernel and clustering methods, in which expensive classical distance calculations are replaced by lower cost quantum computations. The subsequent research explored in \citep{research1}'s paper, provides mathematical designs for these quantum algorithms and quantum logic. The authors do not use quantum circuit implementations to explore and validate their quantum advantage claims. This dissertation provides implementations for the quantum algorithms, and demonstrates how the quantum data encoding is accomplished. It also evaluates these circuits' quantum advantages using a quantum computer. 



\section{A Focus on Implementation}\label{StateImple}

%In this section 3 papers are considered: Sharma, Kok and Qiskit's  --> short intro 
In this section, three papers are considered from the perspective of implementation: a paper by \citeauthor{sharmaQeml}, another paper by \citeauthor{INGKOK} and the Qiskit Tutorial \citep{qiskit_videos_code}. They contribute more implementation details to the quantum algorithms detailed in the publications in the groundworks section. 

The implementations outlined in these works are taken up later in the dissertation, where they are more fully elaborated and evaluated in a modular structure.

%We examine their various approaches and how they enable this dissertation to build on their implemented approaches, by contributing modular implementations for quantum data encoding and the explored quantum algorithms, which are evaluated using both a quantum simulator and a real quantum device.

\subsection{Sharma: Using QEML to implement a kNN Algorithm}
\citep{sharmaQeml}'s \emph{ QEML: (Quantum Enhanced Machine Learning) Using Quantum Computation to implement a K-nearest Neighbors Algorithm in a Quantum Feature Space on Superconducting Processors}, provides \emph{``a side-by-side comparison of both classic ML and QEML''} and makes use of \emph{``tests to compare both the traditional KNN to its quantum variant (QKNN).''} Along with the detailed exploration of QkNN, this research paper provides detailed background and circuit illustrations for Grover’s Search algorithm, as well as the necessary information for a QSVM implementation.  

When detailing QSVM, \citeauthor{sharmaQeml} provides an analysis into how one could implement a QSVM circuit, along with an illustration of a possible circuit implementation. When explaining Grover's Search algorithm, a circuit diagram is provided. However, the diagram is not executed on a quantum machine or on a quantum simulator. Grover's algorithm is only detailed in order to illustrate the speed-up advantage that quantum computers can provide. Section \ref{sec3} of this dissertation details implementations of both QSVM and Grover’s Search algorithm, providing implemented modular circuits for both algorithms. The quantum speed up claims are evaluated using a quantum machine. 

\citeauthor{sharmaQeml}’s work explores a QEML approach of QkNN using a concept known as \emph{fidelity} (discussed in Chapter \ref{BackgroundLit}), as well as a full quantum machine learning approach based on the Hamming distance (also implemented in this dissertation). When explaining fidelity, \citeauthor{sharmaQeml} provides the necessary background into why fidelity could be used instead of the Hamming distance, stating that it \emph{``promotes both faster execution due to quantum algorithms and more storage capabilities due to an enhanced feature space in a quantum register''}. This detailed analysis is accompanied by a Qiskit circuit diagram.
While \citeauthor{sharmaQeml} explains why fidelity would be used, the paper chose to explore the full quantum QkNN approach using the Hamming distance, instead of the quantum enhanced approach of fidelity. When detailing the Hamming distance implementation, \citeauthor{sharmaQeml} provides an overview of the intended circuit diagram and a circuit implementation that was built using the IBM Q experience. 

At the time the paper was written, \citeauthor{sharmaQeml} noted technological limitations as follows: \emph{``physical quantum computers are not accessible to the wide population and are only accessible for researchers and collaborators at recognized institutions.''} These restrictions resulted in none of the circuits in \citeauthor{sharmaQeml}'s work being executed on a quantum computer. The Hamming distance QkNN circuit was instead executed on a quantum simulator. When evaluating the implemented circuits, the Wisconsin Breast Cancer dataset \citep{BCWisconsinData} was used with a dimension of 10. While the research paper does not go into detail about the type of data encoding used, it would appear that each datapoint was mapped to a qubit. As the size of qubit circuits is currently limited to 15 qubits \footnote{For institutional use, the current largest quantum machine available is 50 qubits.}, this type of data encoding limits the number of data points that could be used.

While \citeauthor{sharmaQeml}'s work is a starting point for detailing multiple quantum algorithms in one central location, they do not implement nor provide details of data encoding, and as they were limited in their available resources, they could not provide quantum execution analysis. This dissertation builds on the work of \citeauthor{sharmaQeml} by providing: QkNN, QSVM and Grover’s Search algorithm implementations and two implementations of data encoding. These circuits are evaluated using a quantum simulator and a quantum machine. This dissertation also answers the question posed in \citeauthor{sharmaQeml}'s work: \emph{``how is the performance difference between KNN and QKNN affected when there are n = 100 dimensions or n = 1000 dimensions? Answering these queries will require future analysis.''} The aforementioned quantum circuits are evaluated using the same Wisconsin Breast Cancer dataset, extended to incorporate 10, 100, 150 and 500 datapoints.\footnote{The Wisconsin Breast Cancer dataset contains a total 569 datapoints.} As detailed in Chapter \ref{result}, the quantum circuits in this dissertation are also evaluated using the Iris dataset, taking 10, 100 and 150 datapoints.\footnote{The Iris data contains a total of 150 datapoints.}


%As quantum machines grow in size and become more accessible, we can continue to build on the research of others with the current technology at hand --> possibly future work. 


\subsection{Kok: Building a Quantum kNN Classifier with Qiskit}

In \citep{INGKOK}\emph{ Building a Quantum kNN Classifier with Qiskit: theoretical gains put to practice}, D.J.~Kok implements a version of QkNN using the Dot Product method. The Dot Product method is not a full \emph{quantum} QkNN approach but rather is a hybrid quantum algorithm -- the neighbours are sorted according to a classical method described by \citep{Afham2020} (see also Chapter \ref{BackgroundLit}). \citeauthor{INGKOK} does provide a quantum machine learning implementation that details the data encoding, the quantum circuit and quantum execution with detailed code implementations alongside the necessary background information. While the author details more than one quantum data  encoding technique, only one is implemented. The research paper does not provide multiple quantum machine learning algorithms or data encoding approaches.

The author details data encoding, specifically digital or binary encoding (as explained in Chapter \ref{BackgroundLit}) and analog or amplitude encoding. The paper provides an implementation and detailed explanation of the latter. While implementing a QkNN, the author does not make use of a full quantum machine during classification; rather a classical classifier is used. This classifier reads the quantum data using an oracle, which is executed using a quantum simulator. While \citeauthor{INGKOK} uses multiple datasets to analyse the quantum circuit (the Hofmann (German Credit dataset) \citep{hofmanngerman}, the HEP \citep{hepData} and the Iris datasets \citep{irisData}) a quantum machine is not used to evaluate these circuits, a quantum simulator is used instead. (This dissertation provides two implemented modular data encoding techniques and three quantum algorithms, which are evaluated using both a simulator and a real quantum device.)

In the implementation, the author makes use of the built in Qiskit QkNN function or \emph{'oracle'} to perform the QkNN hybrid classification. (This dissertation provides an implementation of this approach, as well as a full quantum QkNN circuit, both of which are provided in a modular implementation.) 

\citeauthor{INGKOK}'s paper is a more complete example of an implemented and explained approach to a full quantum circuit that encompasses quantum data encoding, a quantum algorithm and a circuit analysis using a quantum computer, all in a central place. %Continuing in this vein
 This dissertation will expand on this and other works mentioned in this literature review chapter, by providing multiple modular implementations for the data encoding, quantum algorithms and circuit analysis.

%#\newpage
\subsection{IBM: Qiskit Machine Learning Tutorial}
At the time of writing, Qiskit provides a central tutorial page for various quantum machine learning algorithms \citep{QML_Tutorial}. It details: \emph{Quantum Neural Networks}, \emph{Neural Network Classifier \& Regressor}, \emph{Quantum Kernel Machine Learning} and \emph{qGANS\footnote{qGAN or Generative Adversarial Network are used to \emph{``learn the data’s underlying random distribution and to load it directly into a quantum state''} \citep{QiskitqGAN}} for Loading Random Distributions} techniques, in a central location. The tutorial site provides the code implementations for these algorithms along with some detail about the implementation process. However, it does not thoroughly detail the data encoding used nor does it execute these circuits on a quantum machine in order to evaluate them further. 

Taking the Quantum Kernel Machine Learning tutorial, the document notes the need for data encoding. Through an illustration, a feature mapping technique (ZZfeature mapping) is specified. However, it does not explain how a general feature map works, rather it depicts its use within the code and links to another document that illustrates the circuit diagram for a ZZfeature map. This subsequent linked document also does not explain the generalised form of a feature map or the necessary background into how a feature map encodes the data.

When investigating this algorithm, the kernel component is similar to the approach used for QSVM (Quantum Support Vector Mechanism). As previously explained, QSVM is a quantum enhanced approach and not a full quantum machine learning algorithm, as the kernel is a classical kernel with quantum encoded data inputs. The Quantum Kernel algorithm in the Qiskit document site makes use of quantum data encoding but again only details it in the same manner as above. Within the implementation code, the quantum algorithm is executed on a quantum simulator. The tutorial site does not provide any detail about the expected execution output or how to interpret the output from the quantum simulator or from a real quantum device.

Qiskit provides introductory quantum videos and tutorials \citep{qiskit_videos_code} centered on how to build a basic quantum circuit. These learning tools briefly explain the quantum background needed \citep{qiskit_videos_background}, but focus more on the code implementation. This tutorial page could be seen as the next step for a quantum enthusiast -- from Qiskit’s basic and introductory quantum circuit tutorials to an actual quantum machine learning implementation. The tutorial provides coded implementations and some explanation for each component of a circuit: data encoding, quantum algorithm and the quantum machine evaluation. However, many steps in the circuit are described in summary form and in many cases there is a need for the algorithm provided to be linked to further explanatory resources. As such, this tutorial site is more of a leap than a next step.% as there is a need for the algorithms provided in the document page to be linked to further explanatory resources and for these circuits to be evaluated on a real quantum device. 

 Chapter \ref{sec3} of this dissertation aims to deliver the necessary background needed to understand how build each component of a quantum machine learning circuit. Each component of the data encoding is explained in the background chapter, Chapter \ref{BackgroundLit}, and each implementation step is explored in Chapter \ref{sec3}. Within Chapter \ref{sec3}, different approaches for data encoding and for quantum machine learning algorithms are provided in a modular form. The dissertation also evaluates each circuit on a quantum simulator and on the IBM quantum computer and the resulting outputs are measured.

